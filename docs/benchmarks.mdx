---
title: "Benchmarks"
description: "Execution latency benchmarks for all isol8 runtimes, including cold start, warm pool, and per-phase breakdowns."
og:title: "Performance Benchmarks - isol8"
og:description: "See how fast isol8 runs code. Latency benchmarks for cold starts vs warm pool execution across Python, Node.js, Bun, and more."
---

isol8 ships with three benchmark scripts that measure end-to-end execution latency for a minimal "hello world" program across all supported runtimes. These benchmarks run on real Docker containers with the same security constraints used in production.

<Info>
Results below were measured on Apple Silicon (Docker Desktop). Your numbers will vary depending on hardware, Docker configuration, and system load.
</Info>

## Running Benchmarks

Benchmarks are available as npm scripts. They require Docker to be running and base images to be built (`isol8 setup`).

```bash
# Cold start — fresh engine per run, no pool reuse
bun run bench

# Warm pool — single engine instance, measures pool speedup
bun run bench:pool

# Detailed breakdown — per-phase timing using raw Docker API
bun run bench:detailed
```

You can also run them directly via `bunx`, `npx`, or `pnpx`:

```bash
bunx tsx benchmarks/spawn.ts
npx tsx benchmarks/spawn.ts
pnpx tsx benchmarks/spawn.ts
```

## Cold Start

Each iteration creates a fresh `DockerIsol8` instance, executes a single "hello world" script, and tears down the engine. This measures worst-case latency when no warm containers are available.

| Runtime | Min | Median | Max | Avg |
|---------|-----|--------|-----|-----|
| Python | 220ms | 280ms | 350ms | 280ms |
| Node.js | 200ms | 250ms | 320ms | 260ms |
| Bun | 180ms | 230ms | 300ms | 230ms |
| Deno | 210ms | 270ms | 340ms | 270ms |
| Bash | 180ms | 220ms | 280ms | 220ms |

**Takeaways:**
- Bash has the fastest cold-start (~220ms median)
- All runtimes achieve sub-350ms max latency
- Python is the slowest cold-start runtime (~280ms median)

```mermaid
---
config:
  xyChart:
    xAxis:
      label: "Runtime"
    yAxis:
      label: "Median Latency (ms)"
---
xychart-beta
    x-axis ["Bash", "Bun", "Deno", "Node.js", "Python"]
    y-axis "Median Latency (ms)" 0 --> 350
    bar [220, 230, 270, 250, 280]
```

## Warm Pool

A single `DockerIsol8` instance is reused across 5 sequential runs. The first run is cold (the pool is empty and a container must be created). Subsequent runs acquire pre-started containers from the warm pool, which eliminates the container create+start overhead.

| Runtime | Cold | Warm Avg | Warm Min | Speedup |
|---------|------|----------|----------|---------|
| Python | 300ms | 160ms | 130ms | 2.3x |
| Node.js | 280ms | 170ms | 140ms | 2.0x |
| Bun | 250ms | 155ms | 130ms | 1.9x |
| Deno | 270ms | 160ms | 140ms | 1.9x |
| Bash | 230ms | 145ms | 125ms | 1.8x |

**Takeaways:**
- The warm pool delivers **1.8-2.3x speedup** for most runtimes
- Python benefits the most (2.3x speedup, 130ms warm minimum)
- Warm execution for all runtimes is consistently **under 170ms**
- Bash is the fastest warm runtime at 125ms minimum

```mermaid
---
config:
  xyChart:
    xAxis:
      label: "Runtime"
    yAxis:
      label: "Latency (ms)"
---
xychart-beta
    x-axis ["Bash", "Bun", "Deno", "Node.js", "Python"]
    y-axis "Latency (ms)" 0 --> 350
    bar [230, 250, 270, 280, 300]
    line [125, 130, 140, 140, 130]
```

<Note>
The bar shows cold start latency. The line shows warm pool minimum. The gap between them is the overhead eliminated by the pool.
</Note>

## Execution Phase Breakdown

This benchmark bypasses `DockerIsol8` and uses the raw `dockerode` API directly to measure time spent in each phase of the container lifecycle. No warm pool is involved.

| Phase | Description |
|-------|-------------|
| **Create** | `docker.createContainer()` — allocates the container |
| **Start** | `container.start()` — boots the container |
| **Write** | Writes user code to `/sandbox/main.*` via `exec` |
| **Exec Setup** | `container.exec()` — creates the exec instance |
| **Run** | `exec.start()` — runs the code and collects output |
| **Cleanup** | `container.remove({ force: true })` — tears down |

| Runtime | Create | Start | Write | Exec Setup | Run | Cleanup | Total |
|---------|--------|-------|-------|------------|-----|---------|-------|
| Python | 69ms | 52ms | 19ms | 1ms | 22ms | 51ms | 213ms |
| Node.js | 47ms | 41ms | 15ms | 1ms | 30ms | 36ms | 169ms |
| Bun | 55ms | 42ms | 15ms | 1ms | 18ms | 37ms | 166ms |
| Bash | 50ms | 50ms | 14ms | 1ms | 13ms | 43ms | 172ms |

<Note>
Deno is excluded from the detailed breakdown because it uses a different base image (`denoland/deno:alpine`) rather than the shared multi-stage build.
</Note>

**Takeaways:**
- **Create + Start** dominate at ~90-120ms combined — this is exactly what the warm pool eliminates
- **Exec Setup** is negligible (~1ms) — Docker exec creation is fast
- **Run** time reflects actual runtime startup: Bash (13ms) and Bun (18ms) are fastest, Python (22ms) and Node.js (30ms) are similar
- **Cleanup** is consistent at ~36-51ms regardless of runtime
- **Write** is fast at ~14-19ms due to base64 encoding via exec

```mermaid
---
config:
  xyChart:
    xAxis:
      label: "Phase"
    yAxis:
      label: "Time (ms)"
---
xychart-beta
    x-axis ["Create", "Start", "Write", "Exec Setup", "Run", "Cleanup"]
    y-axis "Time (ms)" 0 --> 80
    bar [55, 46, 16, 1, 21, 42]
```

## How the Warm Pool Works

The container pool is the primary performance optimization in isol8. Understanding it helps explain the benchmark results.

```mermaid
flowchart LR
    A["execute()"] --> B{Pool has\ncontainer?}
    B -->|Yes| C["Acquire warm\ncontainer"]
    B -->|No| D["Create + start\nnew container"]
    C --> E["Write code\n+ run"]
    D --> E
    E --> F["Collect output"]
    F --> G["Kill sandbox\nprocesses"]
    G --> H["Wipe /sandbox"]
    H --> I["Return to pool"]
    C -.->|Background| J["Replenish pool"]
```

1. **Acquire**: If the pool has a pre-started container for the requested image, it is returned immediately. Otherwise, a new container is created and started inline (cold path).
2. **Execute**: Code is written to `/sandbox/main.*` and executed via `docker exec` as the `sandbox` user.
3. **Release**: After execution, all processes owned by the `sandbox` user are killed (`pkill -9 -u sandbox`), then the container's `/sandbox` tmpfs is wiped clean and the container is returned to the pool for reuse.
4. **Replenish**: When a container is acquired from the pool, a background task creates a replacement container so the pool stays warm for the next request.

The pool supports two strategies: **fast** (default) and **secure**. Fast mode uses a dual-pool system with separate clean and dirty pools for optimal performance. Secure mode performs cleanup synchronously before returning a container, ensuring isolation at the cost of some latency.

The default pool size is `{ clean: 1, dirty: 1 }` per runtime, but this is configurable via the `poolSize` option:

```typescript
// Fast mode (default) - dual-pool system
const fastEngine = new DockerIsol8({
  poolStrategy: "fast",
  poolSize: { clean: 2, dirty: 2 },
});

// Secure mode - cleanup before acquire
const secureEngine = new DockerIsol8({
  poolStrategy: "secure",
  poolSize: 2,
});
```

## Methodology

All benchmarks:
- Execute a minimal "hello world" program (e.g., `print("hello")` for Python, `console.log("hello")` for Node.js)
- Use default security settings: read-only rootfs, network disabled, 512MB memory, 1 CPU, 64 PID limit
- Run inside Docker Desktop on the host machine (no remote Docker)
- Use `performance.now()` for high-resolution timing
- Are located in the `benchmarks/` directory and can be run via `bun run bench`, `bun run bench:pool`, or `bun run bench:detailed`
